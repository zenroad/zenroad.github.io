<!DOCTYPE html>




<html class="theme-next gemini" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/easyicon_net_512.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/animals_horse_flame_outline_32px_543552_easyicon.net.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/animals_horse_flame_outline_16px.png?v=5.1.4">


  <link rel="mask-icon" href="/images/easyicon_net_512.ico?v=5.1.4" color="#222">





  <meta name="keywords" content="3D Semantic Segmentation,paper," />










<meta name="description" content="3D Semantic Segmentation with Submanifold Sparse Convolutional Networks写在前面翻译内容如有不当之处，请多多批评指正。 摘要卷积网络是分析时空数据比如图像、视频、3D形状的事实标准。虽然其中一些数据是天然密集的（比如照片），但还有许多其它的数据本质上是稀疏的。示例包括使用LiDAR扫描器或者RGB-D获得的3D点云。当面对这种稀">
<meta name="keywords" content="3D Semantic Segmentation,paper">
<meta property="og:type" content="article">
<meta property="og:title" content="3D Semantic Segmentation with Submanifold Sparse Convolutional Networks 论文翻译">
<meta property="og:url" content="https://www.zenroad.club/archives/9051cccd.html">
<meta property="og:site_name" content="Zenroad&#39;s Blog">
<meta property="og:description" content="3D Semantic Segmentation with Submanifold Sparse Convolutional Networks写在前面翻译内容如有不当之处，请多多批评指正。 摘要卷积网络是分析时空数据比如图像、视频、3D形状的事实标准。虽然其中一些数据是天然密集的（比如照片），但还有许多其它的数据本质上是稀疏的。示例包括使用LiDAR扫描器或者RGB-D获得的3D点云。当面对这种稀">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://user-images.githubusercontent.com/42510101/44852092-f1670d00-ac94-11e8-8974-e56975f518c2.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/42510101/44852461-e06acb80-ac95-11e8-893c-141ce788197a.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/42510101/44854761-072c0080-ac9c-11e8-9c7c-840d5401e698.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/42510101/44857363-3d6c7e80-aca2-11e8-8714-88bb17353323.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/42510101/44916317-06f93700-ad68-11e8-8987-53e94b5bad36.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/42510101/44916604-c4842a00-ad68-11e8-82dd-d1ab0e1fc855.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/42510101/44854761-072c0080-ac9c-11e8-9c7c-840d5401e698.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/42510101/44946625-8fdca500-ae32-11e8-861a-ed771c666eec.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/42510101/44951153-f68fac00-ae8e-11e8-9cbc-c5c0e2b4dcbd.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/42510101/44951272-baf6e100-ae92-11e8-8090-6d66e15c6be6.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/42510101/44951334-6fddcd80-ae94-11e8-8028-6edb63556889.png">
<meta property="og:updated_time" content="2018-09-02T01:46:34.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="3D Semantic Segmentation with Submanifold Sparse Convolutional Networks 论文翻译">
<meta name="twitter:description" content="3D Semantic Segmentation with Submanifold Sparse Convolutional Networks写在前面翻译内容如有不当之处，请多多批评指正。 摘要卷积网络是分析时空数据比如图像、视频、3D形状的事实标准。虽然其中一些数据是天然密集的（比如照片），但还有许多其它的数据本质上是稀疏的。示例包括使用LiDAR扫描器或者RGB-D获得的3D点云。当面对这种稀">
<meta name="twitter:image" content="https://user-images.githubusercontent.com/42510101/44852092-f1670d00-ac94-11e8-8974-e56975f518c2.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'zfish'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://www.zenroad.club/archives/9051cccd.html"/>





  <title>3D Semantic Segmentation with Submanifold Sparse Convolutional Networks 论文翻译 | Zenroad's Blog</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-124399545-1', 'auto');
  ga('send', 'pageview');
</script>





  <!-- 页面点击小红心 -->
  <script type="text/javascript" src="/js/src/love.js"></script>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Zenroad's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description"></h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/archives/c61c6fd.html" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.zenroad.club/archives/9051cccd.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="zfish">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://user-images.githubusercontent.com/42510101/44462550-8049a900-a647-11e8-8e3b-47c5814a59a5.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zenroad's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">3D Semantic Segmentation with Submanifold Sparse Convolutional Networks 论文翻译</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-08-30T17:33:46+08:00">
                2018-08-30
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper/" itemprop="url" rel="index">
                    <span itemprop="name">Paper</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/archives/9051cccd.html#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="archives/9051cccd.html" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/archives/9051cccd.html" class="leancloud_visitors" data-flag-title="3D Semantic Segmentation with Submanifold Sparse Convolutional Networks 论文翻译">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="3D-Semantic-Segmentation-with-Submanifold-Sparse-Convolutional-Networks"><a href="#3D-Semantic-Segmentation-with-Submanifold-Sparse-Convolutional-Networks" class="headerlink" title="3D Semantic Segmentation with Submanifold Sparse Convolutional Networks"></a>3D Semantic Segmentation with Submanifold Sparse Convolutional Networks</h1><h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h1><p>翻译内容如有不当之处，请多多批评指正。</p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>卷积网络是分析时空数据比如图像、视频、3D形状的事实标准。虽然其中一些数据是天然密集的（比如照片），但还有许多其它的数据本质上是稀疏的。示例包括使用<code>LiDAR</code>扫描器或者<code>RGB-D</code>获得的3D点云。当面对这种稀疏数据时，卷积网络的标准“密集”实现效率非常低。我们引入了新的稀疏卷积运算，旨在更有效地处理空间稀疏数据，并使用它们来构造空间稀疏卷积网络。我们在涉及3D点云的语义分割的两个任务中证明了所得模型的强大性能，称为<code>submanifold sparse convolutional networks（SSCNs）</code>。特别是我们的模型在最近的语义分割竞赛的测试集上优于所有先前的<code>state-of-the=art</code>技术。<br><a id="more"></a></p>
<h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1.介绍"></a>1.介绍</h1><p>卷积网络（ConvNets）构成了涉及分析具有空间和/或时间结构的数据的广泛任务的最先进方法，例如照片，视频或3D表面模型。虽然这些数据通常包括稠密（2D或3D）网格，但其他数据集可能是稀疏的。 例如，手写字体由二维空间中的一维线组成，RGB-D相机制作的图像是三维点云，多边形网格模型在三维空间中由二维表面构成。维度增加，问题也随之出现，具有三维或更多维度的网格上的数据：网格上的点数随其维度呈指数增长。在这种情况下，尽可能利用数据稀疏性以减少数据处理所需的计算资源变得越来越重要。实际上，在分析例如填充点稀疏的4D结构的RGB-D视频时，利用稀疏性是至关重要的。传统的卷积网络实现方法针对填充点稠密网格的数据进行了优化，无法有效处理稀疏数据。最近，业界已经提出了许多卷积网络实现，这些实现被定制用于在稀疏数据上有效工作。在数学上，这些实现中的一些与常规卷积网络相同，但是它们在FLOP与内存方面需要更少的计算资源。以前的工作中，使用<code>im2col</code>操作的稀疏版本，将计算和存储限制为<code>active</code>站点。或使用投票算法用零修剪不必要的乘法。OctNets修改卷积运算符，以在网格中感兴趣区域之外的部分区域中生成<code>averaged</code>隐藏状态。卷积网络的先前稀疏实现的缺点之一是它们通过应用“完全”卷积来“扩展”每层中的稀疏数据。在这项工作中，我们表明可以创建在整个网络中保持相同的稀疏程度的卷积网络。因此，训练具有明显更多层的网络变得切实可行，例如ResNets和DenseNets。为此，我们开发了一种用于执行稀疏卷积（SC）的新方法，并引入了一种新的卷积操作，称为<code>submanifold sparse convolution（SSC）</code>。我们使用这些运算符作为SSCN的基础，其被优化用于3D点云的有效语义分割，例如，图1中所示的示例。<br><img src="https://user-images.githubusercontent.com/42510101/44852092-f1670d00-ac94-11e8-8974-e56975f518c2.png" alt="图1"><br>在表1中，我们在最近的测试集上测试SSCN的性能，并将其与竞争中的一些表现最佳的条目进行比较：SSCN优于所有这些条目。<br><img src="https://user-images.githubusercontent.com/42510101/44852461-e06acb80-ac95-11e8-893c-141ce788197a.png" alt="表1"><br>我们库的源代码可在线公开获取。</p>
<h1 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h1><p>我们的工作主要建立在先前关于稀疏卷积网络的文献和使用密集卷积网络的图像分割的基础上。密集3D卷积在体积数据上的应用实例包括分类和分割;这些方法受限于巨大的内存使用和推理速度过慢，限制了可以使用的模型的大小。人们还开发了用于处理没有像素化的3D点云的方法。考虑到ConvNets在处理2D输入方面的主导地位，这似乎令人惊讶。这可能是由于使用密集的3D卷积网络所涉及的计算量太大。<br>先前关于稀疏卷积的工作实现了卷积运算符，其增加了每层的<code>active sites</code>的数量。在[4]中，所有具有至少一个<code>active</code>输入的<code>site</code>都被视为活动站点。在[3]中，通过使用ReLU和特殊损失函数计算卷积后，获得了更大程度的稀疏性。相比之下，我们引入了子流形稀疏卷积（submanifold sparse convolutions）来固定活动站点的位置，以便稀疏性对于许多层保持不变。我们证明，这使得训练类似于VGG网络或ResNets的深度和高效网络变得切实可行，并且它非常适合于逐点语义分割的任务。OctNets是稀疏卷积的另一种形式。稀疏像素存储在八叉树中：一种数据结构，其中网格立方体逐渐细分为2^3个较小的子立方体，直到子立方体为空或包含单个活动站点。OctNet在空白区域的表面上运行，因此在8×8×8大小的空立方体上进行<code>size-3</code>的OctNet卷积需要23％的密集3D卷积计算。相反，<code>submanifold convolutions</code>不需要在空区域中进行计算。<br>分割点云的另一种方法是避免对输入进行体素化，这可能由于有限的分辨率而导致信息丢失。这可以通过使用特定的数据结构（例如Kd树）来完成，或直接操作无序点。Kd-Networks通过沿着最大变化轴递归地划分空间来构建Kd树，直到树的每个叶子包含一个输入点。N个输入点需要<code>O(Nlog N)</code>的时间。PointNet使用池化操作来生成全局特征向量。完全卷积网络（FCN）作为二维图像分割的方法，利用多个尺度的信息来保存低级信息，以准确描绘对象边界。U-Nets 通过使用卷积来扩展FCN，以便在最终分类阶段之前更准确地将来自不同尺度的信息合并在一起; 见图4。<br><img src="https://user-images.githubusercontent.com/42510101/44854761-072c0080-ac9c-11e8-9c7c-840d5401e698.png" alt="图4"></p>
<h1 id="3-卷积网络的空间稀疏性"><a href="#3-卷积网络的空间稀疏性" class="headerlink" title="3. 卷积网络的空间稀疏性"></a>3. 卷积网络的空间稀疏性</h1><p>我们将d维卷积网络定义为——将(d+1)维张量作为输入的网络：输入张量包含d个时空维度（例如长度，宽度，高度，时间等）和一个附加特征空间维度（例如，RGB颜色通道或表面法向量）。输入对应于d维网格的<code>site</code>，每个<code>site</code>与特征向量相关联。（译者注：<code>site</code>的概念类似于属性元组）。如果特征向量中的任何元素不处于<code>ground state</code>（基态），我们将输入中的站点定义为<code>active</code>，例如，如果它不是非零的。如果数据不是自然稀疏的，则可以使用阈值处理来消除特征向量在距离基态的距离较小的<code>input site</code>。请注意，即使输入张量为(d+1)维，激活向量也是d维的：沿着特征维度，整个向量里的值要么是激活的，要么是非激活的。<br>类似地，d维卷积网络的隐藏层由特征空间矢量的d维网格表示。通过网络传播输入数据时，如果作为输入的图层中的任何<code>site</code>处于活动状态，则隐藏层中的<code>site</code>处于活动状态。（注意，使用卷积大小为3时，每个站点都连接到下面隐藏层中的3^d个站点。）因此，隐藏层中的激活与否遵循归纳定义，其中每个层确定下一层中的活动状态集。在每个隐藏层中，非激活状态的<code>site</code>都具有相同的特征向量：对应于基态（<code>ground state</code>）的向量。基础状态的值仅需要在训练时的每个前进传递计算一次，并且在测试时仅对所有前进传递计算一次。这样可以大大节省计算和内存使用。<br>我们认为上述框架具有不适当的限制性，特别是因为卷积操作尚未被修改以适应输入数据的稀疏性。如果输入数据包含单个<code>active site</code>，则在应用3^d卷积后，将有3^d个<code>active site</code>。应用相同大小的第二个卷积将产生5^d个<code>active site</code>，依此类推。当实现包含数十甚至数百个卷积层的现代卷积网络架构（例如VGG网络，ResNets或DenseNets）时，活动站点数量的快速增长是一个非常糟糕的事。<br>当然，卷积网络通常不应用于仅具有<code>active site</code>的输入，但是当输入数据包括具有两个或更多维度的空间中的一维曲线或者三个或更多维度中的二维表面时，上述膨胀问题同样成问题。 我们将这个问题称为“子流形扩张问题”（“submanifold dilation problem”）。图2说明了这个问题：即使我们在这个网格上应用小的3×3卷积，网格的稀疏性也会迅速消失。<br><img src="https://user-images.githubusercontent.com/42510101/44857363-3d6c7e80-aca2-11e8-8714-88bb17353323.png" alt="图2"></p>
<h1 id="4-Submanifold-Convolutional-Networks"><a href="#4-Submanifold-Convolutional-Networks" class="headerlink" title="4. Submanifold Convolutional Networks"></a>4. Submanifold Convolutional Networks</h1><p>我们探索了一种简单的子流形扩张问题（submanifold dilation problem）解决方案，它将卷积的输出仅限制在一组有效输入点上。这种方法的潜在问题是网络中的隐藏层可能无法接收它们对输入数据进行分类所需的所有信息：特别是，两个相邻的连接组件是完全独立地处理的。我们通过使用包含池化或跨步卷积操作的卷积网络来解决此问题。这种操作在稀疏卷积网络中很重要。因为它们允许信息在输入中的断开连接的组件之间流动。越接近的组件在空间上，组件在其中间表示中“通信”所需的跨步操作越少。</p>
<h2 id="4-1-稀疏的卷积运算"><a href="#4-1-稀疏的卷积运算" class="headerlink" title="4.1 稀疏的卷积运算"></a>4.1 稀疏的卷积运算</h2><p>我们定义了具有m个输入特征平面，n个输出特征平面，f的滤波器大小和步幅s的稀疏卷积SC（m，n，f，s）。SC卷积以与常规卷积相同的方式计算<code>active site</code>集：它在大小为f^d的感知域中查找是否存在任何活动站点。如果输入的大小为l，则输出的大小为（l-f + s）/ s。与常规卷积或其它稀疏卷积不同，SC卷积通过假设来自这些站点的输入为零来丢弃非活动站点的基态。这种看似微小的变化将计算成本降低了大约50％。<br><em>Submanifold sparse convolution.</em> 本文的主要贡献是定义另一种稀疏卷积。设f表示奇数。我们将子流形稀疏卷积SSC（m，n，f）定义为SC（m，n，f，s = 1）卷积的一种变体。首先，我们在每侧填充输入（f - 1）/ 2个零，以便输出与输入具有相同的大小。接下来，如果一个站点对应的输入中相应站点的处于活动状态（即，如果感受野中的中心站点处于活动状态），我们将输出站点限制为活动状态。每当确定输出站点是活动的时，其输出特征向量由SSC卷积计算;有关说明，请参见图3。<br><img src="https://user-images.githubusercontent.com/42510101/44916317-06f93700-ad68-11e8-8987-53e94b5bad36.png" alt="图3"><br>表2显示了常规卷积（C）操作和SC和SSC卷积的计算和内存要求。<br><img src="https://user-images.githubusercontent.com/42510101/44916604-c4842a00-ad68-11e8-82dd-d1ab0e1fc855.png" alt="表2"><br>SSC卷积类似于OctNets，因为它们保留了稀疏结构。但是，与OctNets不同，空白空间在SSC卷积的实现中不会产生计算或内存开销。<br><em>Other operators.</em><br>要使用SC和SSC构建卷积网络，我们还需要激活函数，批量规范化和池化。激活函数和通常定义的一样，但仅应用于活动站点集。同样，我们根据在活动站点集上应用的常规批量规范化（batch normalization）来定义批量规范化。最大池MP（f，s）和平均池AP（f，s）操作被定义为SC（·，·，f，s）的变体。MP在感受野中获取零向量和输入特征向量的最大值。AP计算有效输入向量之和的(f^(-d))倍。我们还将反卷积运算DC（·，·，f，s）定义为SC（·，·，f，s）卷积的逆。来自DC卷积的一组有效输出站点与相应SC卷积的输入有效站点集完全相同：输入和输出站点之间的连接简单地被反转。</p>
<h2 id="4-2-实现"><a href="#4-2-实现" class="headerlink" title="4.2 实现"></a>4.2 实现</h2><p>为了有效地实现（S）SC卷积，我们将输入/隐藏层的状态存储为两部分：哈希表和矩阵。矩阵的大小为a×m，并且每个活动站点包含一行。哈希表包含所有活动站点的（location, row）对：位置是整数坐标的元组，行号表示特征矩阵中的相应行。给定滤波器大小为f的卷积，设F = {0,1，…。。 ，f - 1}^d表示卷积滤波器的空间大小。将规则定义为f^d整数矩阵的集合R = (R i : i ∈ F )，每个矩阵有两列。为了实现SC（m，n，f，s）卷积，我们：<br>1.通过输入哈希表迭代一次。我们通过迭代输入层中的点以及输出层中可以看到它们的所有点来动态构建输出哈希表和规则手册。输出站点是首次访问时，会在输出哈希表中创建一个新条目。对于位于输出y的感受野中的点i处的每个活动输入x，将一行（input-hash（x），output-hash（y））添加到rule book元素R i。<br>2.将输出矩阵初始化为全零。对于每个i∈F，存在参数矩阵Wi∈R^(m×n)。对于R i中的每一行（j，k），将输入特征矩阵的第j行乘以W^i，并将其添加到输出特征矩阵的第k行。这可以在GPU上有效地完成，因为它是matrix-matrix multiply-add。<br>为了实现SSC卷积，我们重新使用输入哈希表作为输出，并构造适当的规则手册。请注意，由于稀疏模式不会更改，因此可以在网络中重复使用相同的规则手册，直到遇到池或子采样层。<br>如果输入层中存在a个活动点，则构建输入散列表的成本为O（a）。对于FCN和U-Net网络，假设每个下采样操作的活动站点数减少乘法因子，构建所有哈希表和规则book的成本也是O（a），无论网络的深度如何。上述实现与[4]的不同之处在于，计算输出位置的成本与有效输入的数量成比例，而不是与感受野的大小成比例。对于SC卷积，这类似于投票算法[22,3] - 滤波器权重永远不会与非活动输入位置相乘。 但是对于SSC卷积，实现的计算密集程度低于投票算法，因为活动输入位置和非活动相邻输出位置之间没有交互。</p>
<h1 id="5-Submanifold-FCNs-and-U-Nets-for-Semantic-Segmentation"><a href="#5-Submanifold-FCNs-and-U-Nets-for-Semantic-Segmentation" class="headerlink" title="5.Submanifold FCNs and U-Nets for Semantic Segmentation"></a>5.Submanifold FCNs and U-Nets for Semantic Segmentation</h1><p>三维语义分割涉及将3D对象或表示为点云的场景分割成其组成部分;必须为输入云中的每个点分配一个分类标签。随着使用卷积网络分割2D图像的进展，最近对3D语义分割问题的兴趣不断增长。特别是通过用于3D对象的基于部件的分割的新数据集以及相关的比赛来推动兴趣。我们使用类似于[3,4]的稀疏体素化输入表示以及SSC卷积和跨步SC卷积的组合来构建流行的FCN和U-Net网络的稀疏变体。得到的网络如图4所示; 有关详细信息，请参阅标题。<br><img src="https://user-images.githubusercontent.com/42510101/44854761-072c0080-ac9c-11e8-9c7c-840d5401e698.png" alt="图4"><br>我们将这些网络称为子流形稀疏卷积网络（SSCN），因为它们处理生活在更高维度空间中的低维数据。<br>我们网络的基本构建模块是“预激活”SSC（·，·，3）卷积。每个卷积之前是批量归一化和ReLU非线性。除了具有标准卷积层的FCN和U-Nets之外，我们还尝试使用这些网络的变体，这些网络使用包含两个SSC（·，3，3）卷积的预激活残余块。这里，剩余连接是标识函数：输入和输出特征的数量相等。每当网络将空间尺度减小两倍时，我们就使用SC（·，·，2,2）卷积。我们对FCN上采样的实现将特征映射到其原始分辨率，而不是使用残差块执行反卷积。这大大减少了FCN中的参数和多重添加操作的数量。</p>
<h1 id="6-实验"><a href="#6-实验" class="headerlink" title="6. 实验"></a>6. 实验</h1><p>在本节中，我们在ShapeNet竞赛数据集上使用SSCN进行实验。我们在性能和计算成本方面将SSCN与三个强基线模型进行比较：（1）形状上下文，（2）密集的3D卷积网络，以及（3）多视图2D卷积网络。在整个实验评估过程中，我们专注于在FLOP中测量的分割精度和计算效率之间的取得平衡。在第二组实验中，我们还研究了NYU深度（v2）数据集上的SSCN性能。</p>
<h2 id="6-1-数据集"><a href="#6-1-数据集" class="headerlink" title="6.1 数据集"></a>6.1 数据集</h2><p>ShapeNet分割数据集包括16个不同的对象类别（平面，椅子，帽子等），每个对象类别由多达6个不同的部分组成。例如，“飞机”被分割成“翅膀”，“引擎”，“身体”和“尾巴”。在所有对象类别中，数据集包含总共50个不同的对象部件类。每个对象表示为3D点云，其通过从底层CAD模型的表面均匀地采样点而获得。每个点云包含2,000到3,000点。为了增加验证集的大小，我们使用点云文件的MD5哈希的第一位重新划分训练集和验证集，以获得具有6,955个示例的训练集和具有7,052个示例的验证集。测试集 包含2,874个例子。在原始数据集中，对象是轴对齐的：例如，火箭总是指向z轴。为了使问题更具挑战性，我们在对每个点云进行分类之前对其进行随机3D平移和旋转。表3中的结果表明，删除对齐确实使分割任务更具挑战性。<br><img src="https://user-images.githubusercontent.com/42510101/44946625-8fdca500-ae32-11e8-861a-ed771c666eec.png" alt="表3"><br>为了评估模型的准确性，我们采用[23]的交叉联合（IoU）度量。针对每个对象类别的每个部分计算IoU，并对该类别的部分和示例进行平均以产生“每个类别的IoU”。这种平均IoU分数的方法奖励这样的模型——即使对于非常小的物体部分也可以进行准确的预测：小部件在精度测量中具有与较大部件相同的重量。通过采用每类别IoU的加权平均值，使用每个类别的训练样本的分数作为权重来获得最终准确度度量。</p>
<h2 id="6-2-实验细节"><a href="#6-2-实验细节" class="headerlink" title="6.2 实验细节"></a>6.2 实验细节</h2><p>在所有实验中，使用相同的数据预处理程序。具体而言，每个点云居中并重新缩放以适合直径为S的球体;scale S确定体素化表示的大小。我们在实验中使用了S∈{16,32,48}。在S = 48的范围内，体素稀疏约99％。在密集卷积网络的实验中，我们将球体随机平移和旋转放置在大小为S的网格中。对于SSCN，我们将球体类似地放置在大小为4S的网格中。为了对点云进行体素化，我们测量每个体素的点数并对它们进行归一化，使得非空体素的平均密度为1。除非另有说明，否则使用相同的优化超参数来训练网络。 我们使用随机梯度下降（SGD），动量为0.9，Nesterov更新，L2权重衰减为10^-4。初始学习率设定为0.1，并且学习率在每轮之后以e^-0.04因子衰减。我们使用16的batch大小训练所有网络的100个轮次。我们在所有40个对象类别上联合使用多级负对数似然损失函数，在所有50个部件标签上训练单个网络。<br>我们尝试了两种类型的SSCN网络架构。第一架构（C3）通过堆叠SSC（·，·，3）卷积在单个空间分辨率上操作;我们使用每层8,16,32或64个过滤器，以及2,4或6层。第二种架构类型包括具有三层下采样的FCN和U-Nets。这些网络在第一层中具有8,16,32或64个过滤器，并且每次数据被下采样时过滤器的数量加倍。对于这些网络中的卷积块，我们使用1,2或3个SSC卷积的堆栈，或者1,2或3个残余块的堆栈。<br>测试细节<br>在测试时，我们仅计算实际出现在被分割的对象中的零件标签的softmax概率，即，我们假设模型知道它们正在分割的对象的类别。不相关部件类的Softmax概率设置为零（并且部件标签上的分布被重新标准化）。对于三种网络类型（C3，FCN和U-Net）中的每一种，我们如上所述训练一系列具有不同尺寸的模型，并在验证集上监控它们的准确性。对于每种网络类型，我们在精度与FLOP曲线中选择对应于局部最大值的网络，并报告这些网络的测试集精度。类似于图像分类中常见的多角度测试，我们在多个视图上集合模型预测：我们通过随机旋转它们来生成对象的k个不同视图，并在对象的k个不同视图上平均每个点的模型预测。</p>
<h2 id="6-3-基准"><a href="#6-3-基准" class="headerlink" title="6.3 基准"></a>6.3 基准</h2><p>除SSCN外，我们在实验中考虑三种基线模型：（1）形状上下文，（2）密集的3D卷积网络，以及（3）多视图2D卷积网络。我们将在下面分别描述四个基线模型的细节。<br>Shape contexts. 受[1]的启发，我们定义了一个体素化的形状上下文向量。具体来说，我们将ShapeContext层定义为SSC（1,27,3,1）子流形卷积运算符的特例：我们将运算符的权重矩阵设置为27×27单位矩阵，以便在其3×3×3邻域中累积体素强度。我们使用大小为2,4,8和16的平均池化来缩放数据，以创建四个额外的视图。结合起来，这为每个体素生成135维特征向量。该特征向量被馈送到具有两个隐藏层的非卷积多层感知器（MLP），接着是50级softmax分类器。MLP每层具有32,64,128,256或512个单元。 在测试时，我们使用k = 3的多视图测试。<br>Dense 3D convolutional networks. 对于密集的3D卷积网络，我们只考虑SSCN网络的密集版本。 由于计算限制，我们将FCN和U-Net卷积块限制为单个C3层。 由于我们在训练期间观察到的数值不稳定性，我们训练了一些模型，学习率降低。 同样，我们使用K = 3多视图测试。<br>Convolutional networks on multi-view 2D projections. 该基线模型通过假设无限焦距将点云投影到二维视图中，在该投影上应用2D卷积网络，并对多个视图上的预测求平均值，从而丢弃数据的固有3D结构。 这种方法的直接优势在于，可以开箱即用的2D视觉模型，无需进一步调整。 此外，计算成本与表面积成比例，而不是点云的体积。在我们实现这种方法时，我们首先将点云转换为大小为S×S×S的3D网格，就像我们对前一个基线所做的那样。然后我们投射到尺寸为S×S的平面，即立方体的面，具有两个特征通道。一个特征通道是沿着相应列的第一个可见的非零体素。 第二通道是到可见体素的距离，归一化到范围[0,2]，类似于RGB-D图像的深度通道。我们的网络架构是上述密集3D卷积网络的二维版本。<br>在训练期间，点云的随机投影被传递到模型中。 落入同一体素的点云中的点被给予相同的预测。 一些体素被其他体遮挡——网络不接收关于被遮挡体素的信息。我们修改多视图测试程序以考虑体素的遮挡。 与之前类似，使用k个随机投影的加权和来执行预测。我们发现2D网络需要更多视图才能获得高精度，并且最多可以使用k = 10个视图。 在2D投影中观察到的体素的权重为1。被遮挡的体素的权重随着到闭塞它们的体素的距离呈指数衰减，这保证了对每个点的预测，即使该点在所有视图中被遮挡。</p>
<h2 id="6-4-结果"><a href="#6-4-结果" class="headerlink" title="6.4 结果"></a>6.4 结果</h2><p>在图5中，我们报告了ShapeNet测试集上的一系列不同大小的变体的平均IoU：（1）三个基线模型和（2）子流形C3，FCN和U-Nets。平均IoU显示为模型用于计算预测所需的乘法 - 加法运算（FLOP）的数量的函数。 请注意，图中的结果与[23]中的结果不能直接比较，因为我们正在更具挑战性的“随机姿势”设置中测试模型。<br><img src="https://user-images.githubusercontent.com/42510101/44951153-f68fac00-ae8e-11e8-9cbc-c5c0e2b4dcbd.png" alt="图5"><br>SSCNs vs. baselines. 图5（a）比较了SSCN与三个基线。结果表明，形状上下文特征，多视图2D ConvNets和密集3D ConvNets在每个FLOP的精度方面大致相同。SSCN网络大幅超越所有基线模型。例如，在10 8 FLOP时，SSCN的平均IoU比基线的平均IoU高6-8％。重要的是，我们的结果表明，限制信息沿数据中的子流形行进并不会妨碍SSCN的性能。确实可以节省大量的计算和内存，可用于训练具有更高精度的大型模型。<br>消融。 在图5（b）中，我们比较了6.2节中介绍的三种SSCN架构。我们观察到涉及下采样和上采样操作（FCN和U-Nets）的SSCN优于在单个空间分辨率上操作的SSCN，并且我们推测这是由于通过下采样获得的增加的感受野。图5（c）显示了SSCN在三种不同尺度S下的性能（使用所有三种架构：C3，FCN和U-Net）。我们观察到SSCN的性能对于不同的S值是相似的，特别是对于低数量的FLOP。 在更高数量的FLOP中，以更大规模运行的模型表现稍好。</p>
<h2 id="6-5-Results-on-Competition-Data"><a href="#6-5-Results-on-Competition-Data" class="headerlink" title="6.5. Results on Competition Data"></a>6.5. Results on Competition Data</h2><p>为了将SSCN与[23]中的竞赛条目进行比较，我们还在对齐的点云上训练了FCN-SSCN。在这个实验中，我们使用随机仿射变换进行数据增强。 我们设置S = 24并在输入层使用64个滤波器，三个下采样级别，每个空间分辨率两个残余块。 将10视图测试的结果与表1中的竞争条目进行比较。测试误差为85.98％，我们的网络优于其他方法≥0.49％的IoU。</p>
<h2 id="6-6-Semantic-Segmentation-of-Scenes"><a href="#6-6-Semantic-Segmentation-of-Scenes" class="headerlink" title="6.6. Semantic Segmentation of Scenes"></a>6.6. Semantic Segmentation of Scenes</h2><p>我们还在NYU深度数据集（v2）上进行了实验，用于场景的语义分割而不是对象。 该数据集包含1,449个RGB-D图像，这些图像在语义上被分割成894个不同的类。 图6显示了来自数据集的两个示例：每个示例包括RGB图像和关联的深度图。 在[6,14]之后，我们裁剪图像并将类的数量减少到40.为了评估模型的性能，我们测量它们的像素分类精度。 我们将模型与2D FCN进行比较。<br><img src="https://user-images.githubusercontent.com/42510101/44951272-baf6e100-ae92-11e8-8090-6d66e15c6be6.png" alt="图6"><br>我们使用两种不同大小的SSCN-FCN网络进行实验。 网络A在输入层有16个滤波器，每级有一个SSC（·，·，3）卷积。网络B在输入层有24个滤波器，每级有两个SSC（·，·，3）卷积。 两个网络都使用八级下采样。 我们在下采样时增加网络中的滤波器数量：特别是，每次我们减小比例时，我们都会添加16（A）或24（B）功能。我们使用深度信息将RGB-D图像转换为3D点云。 云中的每个点都具有标准化为范围[-1,1]的三个（RGB）要素，以及针对点云中的每个点设置为1的第四个指标要素。 需要指示器功能来模拟体素处于活动状态但所有三个颜色通道的值都为零的情况。 在训练中，我们通过对点云应用随机仿射变换来执行数据增强。 在对点云进行体素化之前，我们将比例缩小两倍，并将这些点放入模型的感知区域。我们通过平均对应于体素的点的特征向量来形成体素。 在测试时，我们尝试单视图和多视图预测（即，k = 1且k = 4）。我们在NYU深度数据集（v2）上的实验结果如表4所示。与我们之前的结果一致，表中的结果表明，SSCN在像素精度方面优于2D FCN高达7％。 同时，SSCN也大大降低了预测模型的计算要求。<br><img src="https://user-images.githubusercontent.com/42510101/44951334-6fddcd80-ae94-11e8-8028-6edb63556889.png" alt="表4"><br>为了验证SSCN-FCN-A实际上是否使用深度信息，我们重复上一个实验，同时将所有深度值设置为零; 这可以防止SSCN利用深度信息。 我们观察到：（1）由于活动体素较少，FLOP减少了60％; （2）精度从64.1％下降到50.8％，这表明SSCN在执行分割时确实使用了3D结构。 这证实了SSCN-FCN-A确实使用深度信息进行预测。</p>
<h1 id="7-结论"><a href="#7-结论" class="headerlink" title="7. 结论"></a>7. 结论</h1><p>在本文中，我们引入了子流形稀疏卷积网络（SSCN），用于高维，稀疏输入数据的有效处理。 我们在三维点云的语义分割的一系列实验中证明了SSCN的功效。 具体而言，我们对SSCN网络的实证评估表明，它们在识别物体内的部件和识别较大场景中的物体时，优于一系列最先进的解决此问题的方法。 此外，与替代方法相比，SSCN在计算上是有效的。</p>

      
    </div>
    
    
    
    <div>
    
    <div>

    <div style="text-align:center;color: #ccc;font-size:16px;">-------------End of the article<i class="fa fa-paw"></i>Thank you for reading-------------</div>

</div>

    
    </div>
    <div>    
 
 
    <ul class="post-copyright">
      <li class="post-copyright-author">
          <strong>Author of this article：</strong>zfish
      </li>
      <li class="post-copyright-link">
        <strong>Link to this article：</strong>
        <a href="/archives/9051cccd.html" title="3D Semantic Segmentation with Submanifold Sparse Convolutional Networks 论文翻译">archives/9051cccd.html</a>
      </li>
      <li class="post-copyright-license">
        <strong>Copyright Notice： </strong>
        All articles in this blog, except for special statements, please indicate the source!
      </li>
    </ul>
  
</div>

    <div>
      
        

      
    </div>
    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/3D-Semantic-Segmentation/" rel="tag"># 3D Semantic Segmentation</a>
          
            <a href="/tags/paper/" rel="tag"># paper</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/archives/1d0202a.html" rel="next" title="Install android on VMware">
                <i class="fa fa-chevron-left"></i> Install android on VMware
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/archives/5370e07a.html" rel="prev" title="python3 Python.h No such file or directory">
                python3 Python.h No such file or directory <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="https://user-images.githubusercontent.com/42510101/44462550-8049a900-a647-11e8-8e3b-47c5814a59a5.jpeg"
                alt="zfish" />
            
              <p class="site-author-name" itemprop="name">zfish</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">32</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">16</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#3D-Semantic-Segmentation-with-Submanifold-Sparse-Convolutional-Networks"><span class="nav-number">1.</span> <span class="nav-text">3D Semantic Segmentation with Submanifold Sparse Convolutional Networks</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#写在前面"><span class="nav-number">2.</span> <span class="nav-text">写在前面</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#摘要"><span class="nav-number">3.</span> <span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-介绍"><span class="nav-number">4.</span> <span class="nav-text">1.介绍</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-相关工作"><span class="nav-number">5.</span> <span class="nav-text">2. 相关工作</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-卷积网络的空间稀疏性"><span class="nav-number">6.</span> <span class="nav-text">3. 卷积网络的空间稀疏性</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-Submanifold-Convolutional-Networks"><span class="nav-number">7.</span> <span class="nav-text">4. Submanifold Convolutional Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-稀疏的卷积运算"><span class="nav-number">7.1.</span> <span class="nav-text">4.1 稀疏的卷积运算</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-实现"><span class="nav-number">7.2.</span> <span class="nav-text">4.2 实现</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-Submanifold-FCNs-and-U-Nets-for-Semantic-Segmentation"><span class="nav-number">8.</span> <span class="nav-text">5.Submanifold FCNs and U-Nets for Semantic Segmentation</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#6-实验"><span class="nav-number">9.</span> <span class="nav-text">6. 实验</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#6-1-数据集"><span class="nav-number">9.1.</span> <span class="nav-text">6.1 数据集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-2-实验细节"><span class="nav-number">9.2.</span> <span class="nav-text">6.2 实验细节</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-3-基准"><span class="nav-number">9.3.</span> <span class="nav-text">6.3 基准</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-4-结果"><span class="nav-number">9.4.</span> <span class="nav-text">6.4 结果</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-5-Results-on-Competition-Data"><span class="nav-number">9.5.</span> <span class="nav-text">6.5. Results on Competition Data</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-6-Semantic-Segmentation-of-Scenes"><span class="nav-number">9.6.</span> <span class="nav-text">6.6. Semantic Segmentation of Scenes</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#7-结论"><span class="nav-number">10.</span> <span class="nav-text">7. 结论</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">true</span>

  
</div>



  <div class="footer-custom">混日子真开心</div>


        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://zenroad.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'https://www.zenroad.club/archives/9051cccd.html';
          this.page.identifier = 'archives/9051cccd.html';
          this.page.title = '3D Semantic Segmentation with Submanifold Sparse Convolutional Networks 论文翻译';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://zenroad.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("bmCOyEe5IhQR4sFPnsQeHsNv-gzGzoHsz", "bEku6VeTlemx57pQPypQdwvV");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

<script src="/live2dw/lib/L2Dwidget.min.js?0c58a1486de42ac6cc1c59c7d98ae887"></script><script>L2Dwidget.init({"pluginModelPath":"assets/","model":{"jsonPath":"live2d-widget-model-hibiki"},"display":{"position":"right","width":80,"height":180},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
